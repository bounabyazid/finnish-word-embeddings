{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm as tqdm_notebook\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel,delayed\n",
    "from itertools import islice\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/finnish.pickle')\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.handlers = []\n",
    "sh = logging.StreamHandler()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     42,
     64
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_lines(lines, tokenizer, min_sent_len=5):\n",
    "    \"\"\"Preprocess given lines.\"\"\"\n",
    "    contents = [c \n",
    "                for js in json.loads('[' + ','.join(lines) + ']')\n",
    "                for c in js['content']]\n",
    "    \n",
    "    filter_re = re.compile(r'[^\\w\\s]')\n",
    "    url_re = re.compile(r'\\w+:\\/\\/\\S*')\n",
    "    \n",
    "    sents = []\n",
    "    for doc in contents:\n",
    "        \n",
    "        for sent in sent_tokenizer.tokenize(doc):\n",
    "            sent_tokens = []\n",
    "            \n",
    "            # Additional RE\n",
    "            sent = url_re.sub('<URL>', sent)\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "\n",
    "            # Cleaning up\n",
    "            for token in tokens:\n",
    "                normal_chars = filter_re.sub('', token)\n",
    "                other_chars = filter_re.findall(token)\n",
    "                        \n",
    "                enough_normal_chars = len(normal_chars) > 0\n",
    "                if enough_normal_chars:\n",
    "                    sent_tokens.append(token)\n",
    "                    continue\n",
    "                    \n",
    "                only_one_other = len(other_chars) == 1\n",
    "                others_in_punct = any([c in string.punctuation for c in other_chars])\n",
    "                if only_one_other and not others_in_punct and len(token) > 1:\n",
    "                    sent_tokens.append(token)\n",
    "            \n",
    "            # Add to sentences\n",
    "            if len(sent_tokens) >= min_sent_len:\n",
    "                clean_sent = ' '.join(sent_tokens)\n",
    "                sents.append(clean_sent)\n",
    "    return sents\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"Returns iterator of certain length.\"\"\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "def parallel_preprocess_lines(lines, tokenizer, n_jobs=3, min_sent_len=5):\n",
    "    \"\"\"Parallel preprocessing of lines.\n",
    "    \n",
    "    Args:\n",
    "        lines (list): List of strings that are crawled lines and not in\n",
    "            JSON format yet.\n",
    "        tokenizer (object): \n",
    "        n_jobs (int): Number of parallel workers to use. Defaults to 3.\n",
    "        min_sent_len (int): Minimum number of tokens to be considered\n",
    "            as a sentence. Defaults to 5.\n",
    "            \n",
    "    Returns:\n",
    "        List of unique sentences in an array.\n",
    "    \"\"\"\n",
    "    n = int(np.ceil(len(lines) / n_jobs))\n",
    "    job_lines = [lines[i:i + n] for i in range(0, len(lines), n)]\n",
    "    sent_lists = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(preprocess_lines)(\n",
    "            lines=lines,\n",
    "            tokenizer=tokenizer,\n",
    "            min_sent_len=min_sent_len\n",
    "        )\n",
    "        for lines in job_lines\n",
    "    )\n",
    "    sents = [sent for sent_list in sent_lists for sent in sent_list]\n",
    "    return pd.unique(sents)\n",
    "      \n",
    "def process_file(filepath, tokenizer, out_filepath='../data/processed/test.sl',\n",
    "                 mode='a', lines_per_chunk=30000, min_sent_len=5, n_jobs=3):\n",
    "    with open(filepath, 'r', encoding='utf8') as f:\n",
    "        logger.info('Reading number of lines in the file...')\n",
    "        n_total_lines = sum(1 for _ in f)\n",
    "        f.seek(0)\n",
    "        \n",
    "        with open(out_filepath, mode=mode, encoding='utf8') as fout:\n",
    "            it = grouper(f, lines_per_chunk)\n",
    "            for i,lines in enumerate(it):\n",
    "                start_time = time.perf_counter()\n",
    "                end_line = min((i + 1) * lines_per_chunk, n_total_lines) / 1e3\n",
    "                logger.info(f'\\nLines {i * lines_per_chunk / 1e3:.0f}'\n",
    "                            f' - {end_line:.0f}k'\n",
    "                            f'/ {n_total_lines / 1e3:.0f}k')\n",
    "                \n",
    "                sents = parallel_preprocess_lines(\n",
    "                    [l for l in lines if l], n_jobs=n_jobs,\n",
    "                    tokenizer=tokenizer, min_sent_len=min_sent_len)\n",
    "                \n",
    "                logger.info(f'Writing {len(sents)} sentences...')\n",
    "                fout.write('\\n'.join(sents))\n",
    "                \n",
    "                time_passed = time.perf_counter() - start_time\n",
    "                logger.info(f'Chunk done in {time_passed:.0f} seconds!')\n",
    "                gc.collect()\n",
    "                \n",
    "def process_all_files(in_filedir='../data/feed/',\n",
    "                      out_filepath='../data/processed/all.sl',\n",
    "                      lines_per_chunk=30000,\n",
    "                      create_uncased=True,\n",
    "                      min_sent_len=5,\n",
    "                      tokenizer='tweet',\n",
    "                      n_jobs=3):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    filepaths = [os.path.abspath(p) for p in glob.glob(in_filedir + '*.jl')]\n",
    "    \n",
    "    out_filepath = os.path.abspath(out_filepath)\n",
    "    out_dir = os.path.dirname(out_filepath)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        logger.warn(f'Created directory in {out_dir}')\n",
    "    \n",
    "    if tokenizer == 'tweet':\n",
    "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, \n",
    "                                   preserve_case=True)\n",
    "    else:\n",
    "        raise ValueError('Currently only \"tweet\" tokenizer is supported!')\n",
    "    \n",
    "    for i,path in enumerate(filepaths):\n",
    "        logger.info(f'\\n\\nProcessing file \"{path}\" '\n",
    "                    f'({i + 1} / {len(filepaths)})')\n",
    "        process_file(path, mode='w' if i == 0 else 'a',\n",
    "                     lines_per_chunk=lines_per_chunk,\n",
    "                     out_filepath=out_filepath, min_sent_len=min_sent_len,\n",
    "                     tokenizer=tokenizer,\n",
    "                     n_jobs=n_jobs)\n",
    "        \n",
    "    if create_uncased:\n",
    "        uncased_filepath = os.path.join(\n",
    "            os.path.dirname(out_filepath),\n",
    "            os.path.splitext(out_filepath)[0] + '_uncased.sl')\n",
    "        logger.info(f'Creating uncased into \"{uncased_filepath}\"...')\n",
    "        with open(out_filepath, 'r', encoding='utf8') as f:\n",
    "            with open(uncased_filepath, 'w', encoding='utf8') as fout:\n",
    "                for line in f:\n",
    "                    fout.write(line.lower())\n",
    "                    \n",
    "    logger.info(f'All done in {time.perf_counter() - start_time:.0f} seconds!')\n",
    "    \n",
    "    \n",
    "process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./fwe/data/preprocessed/sents2.csv', 'r', encoding='utf8') as f:\n",
    "    with open('./fwe/data/preprocessed/sents2_lowercase.csv', 'w', encoding='utf8') as fout:\n",
    "        for line in f:\n",
    "            fout.write(line.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = LineSentence('./fwe/data/preprocessed/sents2_lowercase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "    window=5,\n",
    "    size=100,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(sents, progress_per=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v.train(\n",
    "    sents,\n",
    "    total_examples=w2v.corpus_count,\n",
    "    epochs=w2v.epochs,\n",
    "    queue_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('vittu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save('./fwe/data/preprocessed/w2v2_lowercase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.save_word2vec_format('./fwe/data/preprocessed/w2v2_lowercase.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FastText(\n",
    "    size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.build_vocab(sents, progress_per=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.train(\n",
    "    sents,\n",
    "    total_examples=ft.corpus_count,\n",
    "    epochs=ft.epochs,\n",
    "    queue_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save('./fwe/data/preprocessed/ft2_lowercase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.wv.save_word2vec_format('./fwe/data/preprocessed/ft2_lowercase.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'leijona'\n",
    "topn = 40\n",
    "\n",
    "display(ft.wv.most_similar(word, topn=topn))\n",
    "w2v.wv.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "kv = KeyedVectors.load_word2vec_format('./fwe/data/preprocessed/w2v2_lowercase.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kv.vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
